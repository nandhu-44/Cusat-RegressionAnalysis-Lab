{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGl-vkdEujlC",
        "outputId": "f84258fc-bd7c-47ae-b4b2-519845c61f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load the dataset from the locally saved CSV file\n",
        "df = pd.read_csv('breast_cancer_data.csv')\n",
        "\n",
        "# Check the first few rows to verify loading\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Custom function to split data manually into training and testing sets\n",
        "def train_test_split_custom(X, y, test_size=0.2, random_state=None):\n",
        "    if random_state:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # Combine features and labels to shuffle them together\n",
        "    data = np.hstack((X, y.reshape(-1, 1)))\n",
        "\n",
        "    # Shuffle the data\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    # Calculate the split index\n",
        "    split_index = int((1 - test_size) * len(data))\n",
        "\n",
        "    # Split the data into train and test\n",
        "    train_data = data[:split_index, :]\n",
        "    test_data = data[split_index:, :]\n",
        "\n",
        "    # Split into X_train, X_test, y_train, y_test\n",
        "    X_train = train_data[:, :-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, :-1]\n",
        "    y_test = test_data[:, -1]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    # Clip z to avoid overflow in np.exp()\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "# Cost function (negative log likelihood)\n",
        "def cost_function(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)\n",
        "    epsilon = 1e-5\n",
        "    cost = -(1/m) * (y.T @ np.log(h + epsilon) + (1 - y).T @ np.log(1 - h + epsilon))\n",
        "    return cost\n",
        "\n",
        "# Gradient function\n",
        "def gradient(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)\n",
        "    grad = (1/m) * X.T @ (h - y)\n",
        "    return grad\n",
        "\n",
        "# Logistic regression training\n",
        "def logistic_regression(X, y):\n",
        "    # Add intercept term to X\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Initialize parameters (theta)\n",
        "    initial_theta = np.zeros(X.shape[1])\n",
        "\n",
        "    # Minimize the cost function\n",
        "    result = minimize(fun=cost_function, x0=initial_theta, args=(X, y), method='TNC', jac=gradient)\n",
        "\n",
        "    return result.x\n",
        "\n",
        "# Predict function\n",
        "def predict(X, theta):\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept term\n",
        "    probabilities = sigmoid(X @ theta)\n",
        "    return probabilities >= 0.5"
      ],
      "metadata": {
        "id": "DuXj_jPExsc6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data manually\n",
        "X = df.drop(columns=['target']).values\n",
        "y = df['target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split_custom(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "theta = logistic_regression(X_train, y_train)\n",
        "\n",
        "print(\"Trained model parameters:\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0QmsOxqwL9R",
        "outputId": "694c7501-57a9-44da-e4be-d0e2531a6c2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model parameters: [ 2.99271243e+01  1.99234033e+01 -1.03539889e-01  1.08520496e+00\n",
            " -2.20351679e-01 -7.08059707e+02  5.24406914e+02 -9.91309496e+01\n",
            " -5.00491141e+02  3.53381243e+02 -2.53526423e+00 -9.66264084e+00\n",
            " -5.07809775e+00 -5.86854276e+00 -9.37309409e-01 -1.00450669e+02\n",
            "  8.17273743e+02  1.31844666e+01 -1.09480928e+02  3.18754737e+02\n",
            "  5.26023759e+01  2.08893540e+00 -1.88051961e+00  6.06282540e-02\n",
            " -1.43791531e-01  5.24539923e+01 -1.26860721e+02 -3.20621316e+00\n",
            " -3.88481345e+02 -1.12919514e+02 -3.66151510e+02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "predictions = predict(X_test, theta)\n",
        "\n",
        "# Output predictions and actual labels side by side\n",
        "comparison = pd.DataFrame({\n",
        "    'Predicted': predictions.astype(int),\n",
        "    'Actual': y_test.astype(int)\n",
        "})\n",
        "\n",
        "# Print the side-by-side comparison\n",
        "print(comparison.head(20))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhbkGz2jzdoW",
        "outputId": "bc3fe574-bd45-42bf-a392-4159ba4f7d9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Predicted  Actual\n",
            "0           0       0\n",
            "1           0       0\n",
            "2           1       1\n",
            "3           0       0\n",
            "4           0       0\n",
            "5           1       1\n",
            "6           1       1\n",
            "7           1       1\n",
            "8           0       0\n",
            "9           1       1\n",
            "10          0       0\n",
            "11          0       0\n",
            "12          0       0\n",
            "13          1       0\n",
            "14          1       1\n",
            "15          1       1\n",
            "16          0       0\n",
            "17          1       1\n",
            "18          1       1\n",
            "19          0       0\n",
            "\n",
            "Accuracy: 93.86%\n"
          ]
        }
      ]
    }
  ]
}